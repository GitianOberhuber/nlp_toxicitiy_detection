{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "from nltk.corpus import wordnet\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "negation_words = [\"no\", \"not\", \"n't\",\"isnt\", \"wasnt\", \"nor\", \"none\", \"aint\"]\n",
    "filler = [\"be\", \"is\", \"a\", \"an\", \"-pron- \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['comment_text', 'toxicity','target']\n",
    "\n",
    "train_df = pd.read_csv(\"data/pre_processed/train_lemma_nopunct_cleaned_sentencized.csv\", usecols = fields)\n",
    "valid_df = pd.read_csv(\"data/pre_processed/valid_lemma_nopunct_cleaned_sentencized.csv\", usecols = fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "0%...1%...1%...2%...3%...3%...4%...5%...6%...6%...7%...8%...8%...9%...10%...10%...11%...12%...12%...13%...14%...15%...15%...16%...17%...17%...18%...19%...19%...20%...21%...21%...22%...23%...24%...24%...25%...26%...26%...27%...28%...28%...29%...30%...30%...31%...32%...33%...33%...34%...35%...35%...36%...37%...37%...38%...39%...39%...40%...41%...42%...42%...43%...44%...44%...45%...46%...46%...47%...48%...48%...49%...50%...51%...51%...52%...53%...53%...54%...55%...55%...56%...57%...57%...58%...59%...60%...60%...61%...62%...62%...63%...64%...64%...65%...66%...66%...67%...68%...69%...69%...70%...71%...71%...72%...73%...73%...74%...75%...76%...76%...77%...78%...78%...79%...80%...80%...81%...82%...82%...83%...84%...85%...85%...86%...87%...87%...88%...89%...89%...90%...91%...91%...92%...93%...94%...94%...95%...96%...96%...97%...98%...98%...99%...100%...valid\n",
      "0%...3%...6%...8%...11%...14%...17%...19%...22%...25%...28%...30%...33%...36%...39%...42%...44%...47%...50%...53%...55%...58%...61%...64%...66%...69%...72%...75%...78%...80%...83%...86%...89%...91%...94%...97%...100%..."
     ]
    }
   ],
   "source": [
    "dfs = [train_df, valid_df]\n",
    "df_names = [\"train\", \"valid\"]\n",
    "suffix = \"negations_fliped\"\n",
    "path = \"experimental/\"\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    print(df_names[i])\n",
    "    t1 = time.time()\n",
    "    processed_comments = []\n",
    "\n",
    "    for k, text in enumerate(df['comment_text']):\n",
    "        if k % 10000 == 0:\n",
    "            print(\"{0:.0%}...\".format(k/len(dfs[i])), end='')\n",
    "        tokens = word_tokenize(text)\n",
    "        for j, token in enumerate(tokens):\n",
    "            tokenlen = len(tokens)\n",
    "            if (j > 0 and j < (tokenlen - 1) and tokens[j - 1] in negation_words):\n",
    "                if (tokens[j] not in filler):\n",
    "                    tokens[j] = \"NEG_\" + tokens[j]\n",
    "                    del tokens[j - 1]\n",
    "                else:\n",
    "                    if (j < (tokenlen - 2)):\n",
    "                        tokens[j + 1] = \"NEG_\" + tokens[j + 1]\n",
    "                        del tokens[j - 1]\n",
    "\n",
    "        processed_comments.append(\" \".join(tokens))\n",
    "\n",
    "    df['comment_text'] = processed_comments\n",
    "    df.to_csv(path + df_names[i]  + \"_\" + suffix + \".csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "0%...3%...6%...8%...11%...14%...17%...19%...22%...25%...28%...30%...33%...36%...39%...42%...44%...47%...50%...53%...55%...58%...61%...64%...66%...69%...72%...75%...78%...80%...83%...86%...89%...91%...94%...97%...100%...valid\n",
      "0%...11%...22%...33%...44%...55%...66%...78%...89%...100%..."
     ]
    }
   ],
   "source": [
    "#### Alternative Strategy: Add NEG_ to all words after negation until a \".\" is encountered ####\n",
    "if (1 == 0):\n",
    "    dfs = [train_df, valid_df]\n",
    "    df_names = [\"train\", \"valid\"]\n",
    "    suffix = \"negations_fliped\"\n",
    "    path = \"experimental/\"\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        print(df_names[i])\n",
    "        t1 = time.time()\n",
    "        processed_comments = []\n",
    "\n",
    "        pipe = nlp.pipe(df['comment_text'], batch_size = 512 , disable = [\"ner\", \"tagger\"])\n",
    "        results = []\n",
    "        for j, doc in enumerate(pipe):\n",
    "            if k % 10000 == 0:\n",
    "                print(\"{0:.0%}...\".format(k/len(dfs[i])), end='')\n",
    "\n",
    "            processed_comments.append(\" \".join(tokens))\n",
    "\n",
    "        df['comment_text'] = processed_comments\n",
    "        df.to_csv(path + df_names[i]  + \"_\" + suffix + \".csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For experimenting with changing words to their negations (good -> bad) instead of just adding NEG_\n",
    "\n",
    "#returns an antonym if one can be found, None otherwise\n",
    "def find_antonym(word):\n",
    "    antonyms = []\n",
    "    \n",
    "    for syn in wordnet.synsets(word): \n",
    "        for l in syn.lemmas(): \n",
    "            if l.antonyms(): \n",
    "                antonyms.append(l.antonyms()[0].name())\n",
    "    if (len(antonyms) > 0):\n",
    "        return antonyms[0]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "if (1 == 0):\n",
    "    dfs = [train_df, valid_df]\n",
    "    df_names = [\"train\", \"valid\"]\n",
    "    suffix = \"negations_fliped\"\n",
    "    path = \"experimental/\"\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        print(df_names[i])\n",
    "        t1 = time.time()\n",
    "        processed_comments = []\n",
    "\n",
    "        for k, text in enumerate(df['comment_text']):\n",
    "            if k % 10000 == 0:\n",
    "                print(\"{0:.0%}...\".format(k/len(dfs[i])), end='')\n",
    "            tokens = word_tokenize(text)\n",
    "            for j, token in enumerate(tokens):\n",
    "                tokenlen = len(tokens)\n",
    "                if (j > 0 and j < (tokenlen - 1) and tokens[j - 1] in negation_words):\n",
    "                    antonym = find_antonym(token) \n",
    "                    if(antonym != None):\n",
    "                        tokens[j] = antonym\n",
    "                        del tokens[j - 1]\n",
    "            processed_comments.append(\" \".join(tokens))\n",
    "\n",
    "        df['comment_text'] = processed_comments\n",
    "        df.to_csv(path + df_names[i]  + \"_\" + suffix + \".csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tests with dependency parser\n",
    "\n",
    "pipe = nlp.pipe([\"He is a complete idiot\", \\\n",
    "                 \"Do you not think you are an idiot?\", \\\n",
    "                 \"Do not be mean\", \"He is not an idiot, actually he is very smart!\"], batch_size = 512 , disable = [\"ner\", \"tagger\"])\n",
    "results = []\n",
    "for j, doc in enumerate(pipe):\n",
    "    negation_tokens = [tok for tok in doc if tok.dep_ == 'neg']\n",
    "    negation_head_tokens = [token.head for token in negation_tokens]\n",
    "    \n",
    "    print(\" \".join([(\"NEG_\" + tok.text) if (tok in negation_head_tokens) else tok.text for tok in doc]))\n",
    "   \n",
    "doc = nlp.pipe([\"He is not an idiot\"])\n",
    "options = {'compact': True, 'color': 'black', 'font': 'Arial'}\n",
    "displacy.serve(doc, style='dep', options=options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
