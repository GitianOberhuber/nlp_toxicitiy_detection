{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "negation_words = [\"no\", \"not\", \"n't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['comment_text', 'toxicity','target']\n",
    "\n",
    "train_df = pd.read_csv(\"data/pre_processed/subsets/25_percent/train_lemma_nopunct_cleaned_sentencized.csv\", usecols = fields)\n",
    "train_df.dropna(inplace = True)\n",
    "valid_df = pd.read_csv(\"data/pre_processed/subsets/25_percent/valid_lemma_nopunct_cleaned_sentencized.csv\", usecols = fields)\n",
    "valid_df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns an antonym if one can be found, None otherwise\n",
    "def find_antonym(word):\n",
    "    antonyms = []\n",
    "    \n",
    "    for syn in wordnet.synsets(word): \n",
    "        for l in syn.lemmas(): \n",
    "            if l.antonyms(): \n",
    "                antonyms.append(l.antonyms()[0].name())\n",
    "    if (len(antonyms) > 0):\n",
    "        return antonyms[0]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (1 == 0):\n",
    "    dfs = [train_df, valid_df]\n",
    "    df_names = [\"train\", \"valid\"]\n",
    "    suffix = \"negations_fliped\"\n",
    "    path = \"experimental/\"\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        print(df_names[i])\n",
    "        t1 = time.time()\n",
    "        processed_comments = []\n",
    "\n",
    "        for k, text in enumerate(df['comment_text']):\n",
    "            if k % 10000 == 0:\n",
    "                print(\"{0:.0%}...\".format(k/len(dfs[i])), end='')\n",
    "            tokens = word_tokenize(text)\n",
    "            for j, token in enumerate(tokens):\n",
    "                tokenlen = len(tokens)\n",
    "                if (j > 0 and j < (tokenlen - 1) and tokens[j - 1] in negation_words):\n",
    "                    antonym = find_antonym(token) \n",
    "                    if(antonym != None):\n",
    "                        tokens[j] = antonym\n",
    "                        del tokens[j - 1]\n",
    "            processed_comments.append(\" \".join(tokens))\n",
    "\n",
    "        df['comment_text'] = processed_comments\n",
    "        df.to_csv(path + df_names[i]  + \"_\" + suffix + \".csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Alternative Strategy: Just add NEG_ to every negated word instead of replacing it with the opposite ####\n",
    "if (1 == 0):\n",
    "    dfs = [train_df, valid_df]\n",
    "    df_names = [\"train\", \"valid\"]\n",
    "    suffix = \"negations_fliped\"\n",
    "    path = \"experimental/\"\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        print(df_names[i])\n",
    "        t1 = time.time()\n",
    "        processed_comments = []\n",
    "\n",
    "        for k, text in enumerate(df['comment_text']):\n",
    "            if k % 10000 == 0:\n",
    "                print(\"{0:.0%}...\".format(k/len(dfs[i])), end='')\n",
    "            tokens = word_tokenize(text)\n",
    "            for j, token in enumerate(tokens):\n",
    "                tokenlen = len(tokens)\n",
    "                if (j > 0 and j < (tokenlen - 1) and tokens[j - 1] in negation_words):\n",
    "                    tokens[j] = \"NEG_\" + tokens[j]\n",
    "                    del tokens[j - 1]\n",
    "            processed_comments.append(\" \".join(tokens))\n",
    "\n",
    "        df['comment_text'] = processed_comments\n",
    "        df.to_csv(path + df_names[i]  + \"_\" + suffix + \".csv\", index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "0%...3%...6%...8%...11%...14%...17%...19%...22%...25%...28%...30%...33%...36%...39%...42%...44%...47%...50%...53%...55%...58%...61%...64%...66%...69%...72%...75%...78%...80%...83%...86%...89%...91%...94%...97%...100%...valid\n",
      "0%...11%...22%...33%...44%...55%...66%...78%...89%...100%..."
     ]
    }
   ],
   "source": [
    "#### Other Alternative Strategy: Add NEG_ to all words after negation until a \".\" is encountered ####\n",
    "\n",
    "dfs = [train_df, valid_df]\n",
    "df_names = [\"train\", \"valid\"]\n",
    "suffix = \"negations_fliped\"\n",
    "path = \"experimental/\"\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    print(df_names[i])\n",
    "    t1 = time.time()\n",
    "    processed_comments = []\n",
    "    \n",
    "    for k, text in enumerate(df['comment_text']):\n",
    "        if k % 10000 == 0:\n",
    "            print(\"{0:.0%}...\".format(k/len(dfs[i])), end='')\n",
    "        tokens = word_tokenize(text)\n",
    "        for j, token in enumerate(tokens):\n",
    "            tokenlen = len(tokens)\n",
    "            if (j > 0 and j < (tokenlen - 1) and tokens[j - 1] in negation_words):\n",
    "                del tokens[j -1]\n",
    "                while(tokens[j] != \".\" and j < tokenlen):\n",
    "                    tokens[j] = \"NEG_\" + tokens[j]\n",
    "                    j += 1\n",
    "        processed_comments.append(\" \".join(tokens))\n",
    "        \n",
    "    df['comment_text'] = processed_comments\n",
    "    df.to_csv(path + df_names[i]  + \"_\" + suffix + \".csv\", index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
