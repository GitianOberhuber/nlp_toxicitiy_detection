{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import spacy\n",
    "from spacy.tokens import Doc, Token, DocBin\n",
    "from spacy.vocab import Vocab\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from spacy.lang.en import English\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_elongated(text): \n",
    "    return re.sub(r'(?i)(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['comment_text', 'toxicity','target']\n",
    "df_train = pd.read_csv(\"data/raw_split/train_custom.csv\", usecols = fields)\n",
    "df_train.dropna(inplace = True)\n",
    "#df_train = df_train[:10]\n",
    "df_valid = pd.read_csv(\"data/raw_split/valid_custom.csv\", usecols = fields)\n",
    "df_valid.dropna(inplace = True)\n",
    "#df_valid = df_valid[:10]\n",
    "df_test = pd.read_csv(\"data/raw_split/test_custom.csv\", usecols = fields)\n",
    "df_test.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "processing train\n",
      "0%...1%...1%...2%...3%...3%...4%...5%...6%...6%...7%...8%...8%...9%...10%...10%...11%...12%...12%...13%...14%...15%...15%...16%...17%...17%...18%...19%...19%...20%...21%...21%...22%...23%...24%...24%...25%...26%...26%...27%...28%...28%...29%...30%...30%...31%...32%...33%...33%...34%...35%...35%...36%...37%...37%...38%...39%...39%...40%...41%...42%...42%...43%...44%...44%...45%...46%...46%...47%...48%...48%...49%...50%...51%...51%...52%...53%...53%...54%...55%...55%...56%...57%...57%...58%...59%...60%...60%...61%...62%...62%...63%...64%...64%...65%...66%...66%...67%...68%...69%...69%...70%...71%...71%...72%...73%...73%...74%...75%...75%...76%...77%...78%...78%...79%...80%...80%...81%...82%...82%...83%...84%...85%...85%...86%...87%...87%...88%...89%...89%...90%...91%...91%...92%...93%...94%...94%...95%...96%...96%...97%...98%...98%...99%...100%...995.6503140926361\n",
      "train\n",
      "\n",
      "processing valid\n",
      "0%...3%...6%...8%...11%...14%...17%...19%...22%...25%...28%...30%...33%...36%...39%...42%...44%...47%...50%...53%...55%...58%...61%...64%...66%...69%...72%...75%...78%...80%...83%...86%...89%...91%...94%...97%...100%...250.8466763496399\n",
      "valid\n",
      "\n",
      "processing test\n",
      "0%...5%...10%...15%...21%...26%...31%...36%...41%...46%...51%...57%...62%...67%...72%...77%...82%...87%...92%...98%...130.74191856384277\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "dfs = [df_train, df_valid, df_test]\n",
    "df_names = [\"train\", \"valid\", \"test\"]\n",
    "suffix = \"lemma_nopunctExceptExcl_cleaned_sentencized\"\n",
    "path = \"data/pre_processed/\"\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    print(\"\\nprocessing\", df_names[i])\n",
    "    t1 = time.time()\n",
    "    processed_comments = []\n",
    "    df.dropna(subset = ['comment_text'], inplace = True)\n",
    "    for text in df['comment_text']:\n",
    "        text = re.sub(r'[-()\\\"#/@;:<>{}=~|,]',\" \", text)\n",
    "        text = re.sub(r\"\\n\\r\",\"\", text)\n",
    "        combine_whitespaces = re.compile(r\"\\s+\")\n",
    "        processed_comments.append(reduce_elongated(combine_whitespaces.sub(\" \", text).strip()))\n",
    "\n",
    "    pipe = nlp.pipe(processed_comments, batch_size = 512 ,disable = [\"tagger\", \"parser\", \"ner\"])\n",
    "    results = []\n",
    "    for j, doc in enumerate(pipe):\n",
    "        s = []\n",
    "        if j % 10000 == 0:\n",
    "            print(\"{0:.0%}...\".format(j/len(dfs[i])), end='')\n",
    "        for sent in list(doc.sents):\n",
    "            s.extend([token.lemma_ for token in sent if not token.is_digit\\\n",
    "                                  and not token.like_url and not token.like_email and not token.like_num])\n",
    "            s.extend(['.'])            \n",
    "        results.append(\" \".join(s))\n",
    "    \n",
    "   \n",
    "    t2 = time.time()\n",
    "    print(t2- t1)\n",
    "\n",
    "    df['comment_text'] = results\n",
    "    print(df_names[i])\n",
    "    df.to_csv(path + df_names[i]  + \"_\" + suffix + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
