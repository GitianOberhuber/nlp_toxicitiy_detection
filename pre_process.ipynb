{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import spacy\n",
    "from spacy.tokens import Doc, Token, DocBin\n",
    "from spacy.vocab import Vocab\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from spacy.lang.en import English\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "nlp.Defaults.stop_words = {\"a\", \"this\", \"that\", \"it\" \"its\", \"and\", \"be\", \"to\", \"just\", \"the\", \"on\", \"but\", \"in\", \"for\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_elongated(text): \n",
    "    return re.sub(r'(?i)(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['comment_text', 'toxicity','target', 'id']\n",
    "df_train = pd.read_csv(\"data/raw_split/train_custom.csv\", usecols = fields)\n",
    "df_train.dropna(inplace = True)\n",
    "#df_train = df_train[:100]\n",
    "df_valid = pd.read_csv(\"data/raw_split/valid_custom.csv\", usecols = fields)\n",
    "df_valid.dropna(inplace = True)\n",
    "#df_valid = df_valid[:100]\n",
    "df_test = pd.read_csv(\"data/raw_split/test_custom.csv\", usecols = fields)\n",
    "df_test.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "processing valid\n",
      "0%...3%...6%...8%...11%...14%...17%...19%...22%...25%...28%...30%...33%...36%...39%...42%...44%...47%...50%...53%...55%...58%...61%...64%...66%...69%...72%...75%...78%...80%...83%...86%...89%...91%...94%...97%...100%...740.7140009403229\n",
      "valid\n"
     ]
    }
   ],
   "source": [
    "#dfs = [df_train, df_valid, df_test]\n",
    "#df_names = [\"train\", \"valid\", \"test\"]\n",
    "dfs = [df_valid]\n",
    "df_names = [\"valid\"]\n",
    "suffix = \"lemma_nopunct_cleaned_sentencized_with_id\"\n",
    "path = \"data/pre_processed/\"\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    print(\"\\nprocessing\", df_names[i])\n",
    "    t1 = time.time()\n",
    "    processed_comments = []\n",
    "    df.dropna(subset = ['comment_text'], inplace = True)\n",
    "    for text in df['comment_text']:\n",
    "        text = re.sub(r'!?[-()\\\"#/@;:<>{}=~|,]',\" \", text)\n",
    "        text = re.sub(r\"\\*\\n\\r\",\"\", text)\n",
    "        combine_whitespaces = re.compile(r\"\\s+\")\n",
    "        processed_comments.append(reduce_elongated(combine_whitespaces.sub(\" \", text).strip()))\n",
    "\n",
    "    pipe = nlp.pipe(processed_comments, batch_size = 512 ,disable = [\"ner\", \"tagger\", \"parser\"])\n",
    "    results = []\n",
    "    for j, doc in enumerate(pipe):\n",
    "        s = []\n",
    "        if j % 10000 == 0:\n",
    "            print(\"{0:.0%}...\".format(j/len(dfs[i])), end='')\n",
    "        for sent in list(doc.sents):\n",
    "            s.extend([token.lemma_ for token in sent \\\n",
    "                      if not token.is_digit\\\n",
    "                      and not token.text == \".\" and not token.like_url and not token.like_email \\\n",
    "                      and not token.like_num and not token.is_punct])\n",
    "            \n",
    "            #---For stopword removal and person name masking\n",
    "             #and not token.lemma_ in nlp.Defaults.stop_words \\\n",
    "             #(\"-PERSON-\" if token.ent_type_ == \"PERSON\" else token.lemma_)\n",
    "            s.extend(['.'])            \n",
    "        results.append(\" \".join(s))\n",
    "    \n",
    "   \n",
    "    t2 = time.time()\n",
    "    print(t2- t1)\n",
    "\n",
    "    df['comment_text'] = results\n",
    "    print(df_names[i])\n",
    "    df.to_csv(path + df_names[i]  + \"_\" + suffix + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEs: -PERSON-, -PERSON-, be, rather, mean\n",
      "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\n",
      "['you', 'b', '*', 'tch']\n"
     ]
    }
   ],
   "source": [
    "#cell for testing things\n",
    "\n",
    "import editdistance\n",
    "\n",
    "editdistance.eval('idiotism', 'idiots')\n",
    "\n",
    "pipe = nlp.pipe([\"Sara Fredericks was rather mean\"], batch_size = 512 ,disable = [\"tagger\", \"parser\"])\n",
    "for j, doc in enumerate(pipe):\n",
    "    print(\"NEs: \" + \", \".join([(\"-PERSON-\" if token.ent_type_ == \"PERSON\" else token.lemma_) for token in doc]))\n",
    "    \n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sa = SentimentIntensityAnalyzer()\n",
    "sentiments = sa.polarity_scores(\"Donald Trump\")\n",
    "print(sentiments)\n",
    "\n",
    "print(word_tokenize(\"b*tch\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
