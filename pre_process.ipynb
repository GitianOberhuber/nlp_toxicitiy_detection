{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import spacy\n",
    "from spacy.tokens import Doc, Token, DocBin\n",
    "from spacy.vocab import Vocab\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from spacy.lang.en import English\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'<LINK>', text)\n",
    "\n",
    "def reduce_elongated(text): \n",
    "    return re.sub(r'(?i)(.)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['comment_text', 'toxicity','target']\n",
    "df_train = pd.read_csv(\"data/train_custom.csv\", usecols = fields)\n",
    "df_train = df_train[:int(len(df_train) * 1/4)]\n",
    "#df_train = df_train[:10]\n",
    "df_valid = pd.read_csv(\"data/valid_custom.csv\", usecols = fields)\n",
    "df_valid = df_valid[:int(len(df_valid) * 1/4)]\n",
    "#df_valid = df_valid[:10]\n",
    "#df_test = pd.read_csv(\"data/test_custom.csv\", usecols = fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "processing train\n",
      "0%...3%...6%...8%...11%...14%...17%...19%...22%...25%...28%...30%...33%...36%...39%...42%...44%...47%...50%...53%...55%...58%...61%...64%...66%...69%...72%...75%...78%...80%...83%...86%...89%...91%...94%...97%...100%...782.5173802375793\n",
      "train\n",
      "\n",
      "processing valid\n",
      "0%...11%...22%...33%...44%...55%...66%...78%...89%...100%...212.74158692359924\n",
      "valid\n"
     ]
    }
   ],
   "source": [
    "#dfs = [df_train, df_valid, df_test]\n",
    "dfs = [df_train, df_valid]\n",
    "#df_names = [\"train\", \"valid\", \"test\"]\n",
    "df_names = [\"train\", \"valid\"]\n",
    "suffix = \"lemma_nopunct_cleaned_sentencized\"\n",
    "path = \"data/pre_processed/subsets/25_percent/\"\n",
    "\n",
    "for i, df in enumerate(dfs):\n",
    "    print(\"\\nprocessing\", df_names[i])\n",
    "    t1 = time.time()\n",
    "    processed_comments = []\n",
    "    df.dropna(subset = ['comment_text'], inplace = True)\n",
    "    for text in df['comment_text']:\n",
    "        text = re.sub(r'[-()\\\"#/@;:<>{}=~|,]',\" \", text)\n",
    "        text = re.sub(r\"\\n\\r\",\"\", text)\n",
    "        combine_whitespaces = re.compile(r\"\\s+\")\n",
    "        processed_comments.append(reduce_elongated(combine_whitespaces.sub(\" \", text).strip()))\n",
    "\n",
    "    pipe = nlp.pipe(processed_comments, batch_size = 512 ,disable = [\"tagger\", \"parser\"])\n",
    "    results = []\n",
    "    for j, doc in enumerate(pipe):\n",
    "        s = []\n",
    "        if j % 10000 == 0:\n",
    "            print(\"{0:.0%}...\".format(j/len(dfs[i])), end='')\n",
    "        for sent in list(doc.sents):\n",
    "            s.extend([token.lemma_.lower() for token in sent if not token.is_punct and not token.is_digit\\\n",
    "                                  and not token.like_url and not token.like_email and not token.like_num])\n",
    "            s.extend(['.'])            \n",
    "        results.append(\" \".join(s))\n",
    "    \n",
    "   \n",
    "    t2 = time.time()\n",
    "    print(t2- t1)\n",
    "\n",
    "    df['comment_text'] = results\n",
    "    print(df_names[i])\n",
    "    df.to_csv(path + df_names[i]  + \"_\" + suffix + \".csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yana stinkt sehr stark ..']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
