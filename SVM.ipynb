{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic comment detection using SVM and different vectorization features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/krise/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/krise/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/krise/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import time\n",
    "from sklearn.preprocessing import normalize\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import preprocessing\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "#List of profane words \n",
    "profane_list = list(pd.read_csv(\"data/profane_list2.csv\")['name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code based on https://github.com/bmeaut/python_nlp_2018_spring/blob/master/course_material/14_Semantics_II/14_Semantics_2_lab.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom featurizer calss\n",
    "class Featurizer():\n",
    "    sa = SentimentIntensityAnalyzer() #vader sentiment analyzer for sentence-sentiment feature\n",
    "    feature_functions = [] #for holding function names that the Featurizer shall apply\n",
    "    \n",
    "    #potential feature functions:\n",
    "    @staticmethod\n",
    "    def number_of_profanities(text):\n",
    "        n = 0\n",
    "        words_tokenized = word_tokenize(text)\n",
    "        for word in words_tokenized:\n",
    "            if (word in profane_list):\n",
    "                n += 1\n",
    "        yield (\"number of profanities\", n / len(words_tokenized))  \n",
    "        \n",
    "    @staticmethod\n",
    "    def pos_tags(text):\n",
    "        word_tag_tuples = nltk.pos_tag(word_tokenize(text))\n",
    "        tag_count_dict = {}\n",
    "        for word_tag_tuple in word_tag_tuples:\n",
    "            if(word_tag_tuple[1] in tag_count_dict.keys()):\n",
    "                tag_count_dict[word_tag_tuple[1]] += 1\n",
    "            else:\n",
    "                tag_count_dict[word_tag_tuple[1]] = 1\n",
    "\n",
    "        for tag in tag_count_dict:\n",
    "            yield (tag, tag_count_dict[tag] / len(word_tag_tuples))\n",
    "\n",
    "    @staticmethod\n",
    "    def vader_sentiment(text):\n",
    "        sentiments = Featurizer.sa.polarity_scores(text)\n",
    "        for key in sentiments:\n",
    "            yield(\"vader \" + key, sentiments[key])\n",
    "            \n",
    "    @staticmethod\n",
    "    def comment_length(text):\n",
    "        yield (\"text length\", len(text))\n",
    "            \n",
    "    @staticmethod\n",
    "    def percentage_uppercase(text):\n",
    "        yield (\"% uppercase\", sum(1 for c in text if c.isupper()) /len(text))\n",
    "            \n",
    "    @staticmethod\n",
    "    def pos_ngrams(text):\n",
    "        n = 2\n",
    "        tokens = word_tokenize(text)\n",
    "        tag_count_dict = {}\n",
    "        \n",
    "        l = len(tokens)\n",
    "        word_tag_tuples = nltk.pos_tag(tokens)\n",
    "        for x in range(0, l - n + 1):\n",
    "            pos_ngram = \"\"\n",
    "            for i in range(0, n):\n",
    "                pos_ngram += word_tag_tuples[x + i][1] + \" \"\n",
    "            if(pos_ngram in tag_count_dict.keys()):\n",
    "                tag_count_dict[pos_ngram] += 1\n",
    "            else:\n",
    "                tag_count_dict[pos_ngram] = 1\n",
    "\n",
    "        for tag in tag_count_dict:\n",
    "            yield (tag, tag_count_dict[tag] / (len(word_tag_tuples) / 2))\n",
    "            \n",
    "    @staticmethod\n",
    "    def char_1grams(text):\n",
    "        chars = set(text)\n",
    "        for char in chars:\n",
    "            yield(\"c1: \" + \"\".join(char), text.count(char) / len(text))\n",
    "            \n",
    "    @staticmethod\n",
    "    def char_2grams(text):\n",
    "        bigramset = set()\n",
    "        for ngram in ngrams(text, 2):\n",
    "            bigramset.add(ngram)\n",
    "\n",
    "        for bigram in bigramset:\n",
    "            bigramstring = \"\".join(bigram)\n",
    "            yield (\"c2: \" + bigramstring, text.count(bigramstring) / (len(text) / 2))\n",
    "            \n",
    "    @staticmethod\n",
    "    def char_3grams(text):\n",
    "        trigramset = set()\n",
    "        for ngram in ngrams(text, 3):\n",
    "            trigramset.add(ngram)\n",
    "\n",
    "        for trigram in trigramset:\n",
    "            trigramstring = \"\".join(trigram)\n",
    "            yield (\"c3: \" + trigramstring, text.count(trigramstring) / (len(text) / 3))\n",
    "\n",
    "    #Either start completely new feature/id dictionary if only features of this custom featurizer will be used\n",
    "    #or base dictionary on that of a different featurizer so they can be used together (e.g sklearns TFIDF vectorizer)\n",
    "    def __init__(self, foreign_features = None):\n",
    "        if (foreign_features == None): #Will create standalone dictionaries\n",
    "            self.features = {}\n",
    "            self.features_by_id = {}\n",
    "            self.next_feature_id = 0\n",
    "            self.max_foreign_feature = 0\n",
    "        else: #Some other vectorizer will be used in addition to this one\n",
    "            self.features = foreign_features\n",
    "            self.features_by_id = {v: k for k, v in foreign_features.items()}\n",
    "            self.next_feature_id = max(foreign_features.values()) + 1\n",
    "            self.max_foreign_feature = self.next_feature_id\n",
    "\n",
    "    def to_sparse(self, events):\n",
    "        \"\"\"convert sets of ints to a scipy.sparse.csr_matrix\"\"\"\n",
    "        data, row_ind, col_ind = [], [], []\n",
    "        for event_index, event in enumerate(events):\n",
    "            for feature, value in event:\n",
    "                if (value != None):\n",
    "                    data.append(value)\n",
    "                else:\n",
    "                    data.append(1)\n",
    "                    \n",
    "                row_ind.append(event_index)\n",
    "                #foreign features will be 0 if only this featurizer is used\n",
    "                col_ind.append(feature - self.max_foreign_feature) \n",
    "                \n",
    "        n_features = len(self.features.keys()) - self.max_foreign_feature\n",
    "        n_events = len(events)\n",
    "        matrix = scipy.sparse.csr_matrix(\n",
    "            (data, (row_ind, col_ind)), shape=(n_events, n_features))\n",
    "        return matrix\n",
    "\n",
    "    def featurize(self, dataset, allow_new_features=False, verbose = False):\n",
    "        events, labels = [], []\n",
    "        n_events = len(dataset)\n",
    "        for c, (text, label) in enumerate(dataset):\n",
    "            if (verbose):\n",
    "                if c % 10000 == 0:\n",
    "                    print(\"{0:.0%}...\".format(c/n_events), end='')\n",
    "            labels.append(label)\n",
    "            events.append(set())\n",
    "            for function_name in Featurizer.feature_functions:\n",
    "                function = getattr(Featurizer, function_name)\n",
    "                for feature, value in function(text):\n",
    "                    if feature not in self.features:\n",
    "                        if not allow_new_features:\n",
    "                            continue\n",
    "                        self.features[feature] = self.next_feature_id\n",
    "                        self.features_by_id[self.next_feature_id] = feature\n",
    "                        self.next_feature_id += 1\n",
    "                    feat_id = self.features[feature]\n",
    "                    events[-1].add((feat_id, value))\n",
    "                    \n",
    "        events_sparse = self.to_sparse(events)\n",
    "        labels_array = np.array(labels)\n",
    "        print('done!')\n",
    "        \n",
    "        #min_max_scaler = preprocessing.MaxAbsScaler()\n",
    "        #events_sparse = min_max_scaler.fit_transform(events_sparse)\n",
    "\n",
    "        return events_sparse, labels_array\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return list([k for k, v in sorted(self.features.items(), key=lambda item: item[1])])\n",
    "    \n",
    "    def get_featurname_to_id_dict(self):\n",
    "        return self.features\n",
    "        \n",
    "    def get_id_to_featurname_dict(self):\n",
    "        return self.features_by_id\n",
    "        \n",
    "    #merge two feature matrices\n",
    "    def merge_feature_matrices(self, foreign_matrix, inherent_matrix):\n",
    "        return sp.hstack([foreign_matrix, inherent_matrix])\n",
    "    \n",
    "    def print_sample_with_feature_names(self, feature_matrix, sample_id):\n",
    "        sample = feature_matrix.getrow(sample_id)\n",
    "        nonzero_idxs = sample.nonzero()[1]\n",
    "        for idx in nonzero_idxs:\n",
    "            print(self.features_by_id[idx].rjust(30, ' ') , \":\" , sample.getcol(idx).toarray()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting most important features for SVM in a plot(works only for linear kernel)\n",
    "def plot_coefficients(classifier, feature_names, top_features=20):\n",
    "    #feature_names*=3\n",
    "    coef = classifier.coef_.ravel()\n",
    "    top_positive_coefficients = np.argsort(-coef)[:top_features]\n",
    "    top_negative_coefficients = np.argsort(-coef)[-top_features:]\n",
    "    top_coefficients = np.hstack([top_positive_coefficients, top_negative_coefficients])\n",
    "    # create plot\n",
    "    plt.figure(figsize=(3, 10))\n",
    "    colors = ['green' if c < 0 else 'red' for c in coef[top_coefficients]]\n",
    "    plt.barh(np.arange(2 * top_features), coef[top_coefficients], color=colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.yticks(np.arange(0,2 * top_features), feature_names[top_coefficients], ha='right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best results so far\n",
    "base = {\"1 precision\": 0.66, \"1 recall\" : 0.58, \"1 f-score\" :0.61, \"0 precision\": 0.96, \\\n",
    "        \"0 recall\" : 0.97, \"0 f-score\" :0.97, \"acc\" : 0.9422, \"Av.rec\": 0.5169, \"auc\": 0.8101}\n",
    "\n",
    "#print results and save some examples of correctly and incorrectly classified comments to disk\n",
    "def evaluate(predictions, dev_labels, valid_df, model, featurizer, plot):\n",
    "    stats_by_label = defaultdict(lambda: defaultdict(int))\n",
    "    if (isinstance(predictions, np.ndarray)):\n",
    "        predictions = predictions.tolist()\n",
    "    for i, gold in enumerate(dev_labels):\n",
    "        auto = predictions[i]\n",
    "        # print(auto, gold)\n",
    "        if auto == gold:\n",
    "            stats_by_label[auto]['tp'] += 1\n",
    "        else:\n",
    "            stats_by_label[auto]['fp'] += 1\n",
    "            stats_by_label[gold]['fn'] += 1\n",
    "\n",
    "    print(\"{:>8} {:>8}  {:>8}    {:>8}         {:>8}             {:>8}\".format(\n",
    "        'label', 'n_true', 'n_tagged', 'precision', 'recall', 'F-score'))\n",
    "    for label, stats in stats_by_label.items():\n",
    "        all_tagged = stats['tp'] + stats['fp']\n",
    "        stats['prec'] = stats['tp'] / all_tagged if all_tagged else 0\n",
    "        all_true = stats['tp'] + stats['fn']\n",
    "        stats['rec'] = stats['tp'] / all_true if all_true else 0\n",
    "        stats['f'] = (2 / ((1/stats['prec']) + (1/stats['rec']))\n",
    "                      if stats['prec'] > 0 and stats['rec'] > 0 else 0)\n",
    "\n",
    "        print(\"{:>8} {:>8} {:>8} {:>8.2f} / {:<8.2f} {:>8.2f} / {:<8.2f} {:>8.2f} / {:<8.2f}\".format(\n",
    "            label, all_true, all_tagged, stats['prec'], base[str(label) + \" precision\"], stats['rec'], \n",
    "            base[str(label) + \" recall\"], stats['f'], base[str(label) + \" f-score\"]))\n",
    "\n",
    "    accuracy = (\n",
    "        sum([stats_by_label[label]['tp'] for label in stats_by_label]) /\n",
    "        len(predictions)) if predictions else 0\n",
    "\n",
    "    av_rec = sum([stats['rec'] for stats in stats_by_label.values()]) / 3\n",
    "    f_pn = (stats_by_label['positive']['f'] +\n",
    "            stats_by_label['negative']['f']) / 2\n",
    "\n",
    "    print()\n",
    "    print(\"{:>10} {:>.4f} / {:<.4f}\".format('Acc:', accuracy, base[\"acc\"]))\n",
    "    print(\"{:>10} {:>.4f} / {:<.4f}\".format('Av.rec:', av_rec, base[\"Av.rec\"]))\n",
    "    print(\"{:>10} {:>.4f} / {:<.4f}\".format('AUC :', roc_auc_score(predictions, dev_labels), base[\"auc\"]))\n",
    "    print(\"-----------------------------\")\n",
    "    \n",
    "    incorrectly_classified_msk = [ x != y for (x,y) in zip(predictions, list(valid_df['target']))]\n",
    "    correctly_classified_msk = [not x for x in incorrectly_classified_msk]\n",
    "    incorrectly_classified = valid_df[incorrectly_classified_msk]\n",
    "    correctly_classified = valid_df[correctly_classified_msk]\n",
    "    \n",
    "    incorrectly_classified_ambig = incorrectly_classified.loc[(incorrectly_classified['toxicity'] > 0.4)\\\n",
    "                                                              & (incorrectly_classified['toxicity'] < 0.6)] \n",
    "    print(\"From\", len(incorrectly_classified), \"incorrectly classified,\", len(incorrectly_classified_ambig),\\\n",
    "          \"are ambiguous\", \"(\", len(incorrectly_classified_ambig) / len(incorrectly_classified) *100, \"%)\")\n",
    "    \n",
    "    incorrectly_classified.loc[(incorrectly_classified['toxicity'] <= 0.4)\\\n",
    "                                | (incorrectly_classified['toxicity'] >= 0.6)].head(100).to_csv(\"examples/incorrect.csv\")\n",
    "    correctly_classified.loc[(correctly_classified['toxicity'] <= 0.4)\\\n",
    "                                | (incorrectly_classified['toxicity'] >= 0.6)].head(100).to_csv(\"examples/correct.csv\")\n",
    "    print(\"saved (non-ambiguous) examples for correctly and incorrectly classified comments to /examples\" )\n",
    "    \n",
    "    if (plot):\n",
    "        plot_coefficients(model, featurizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in data with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = ['comment_text', 'toxicity','target']\n",
    "\n",
    "train_df = pd.read_csv(\"data/pre_processed/train_lemma_nopunct_cleaned_sentencized.csv\", usecols = fields)\n",
    "#train_df = pd.read_csv(\"experimental/train_negations_fliped.csv\", usecols = fields)\n",
    "train_df = train_df[:int(len(train_df) * 1/4)]\n",
    "valid_df = pd.read_csv(\"data/pre_processed/valid_lemma_nopunct_cleaned_sentencized.csv\", usecols = fields)\n",
    "#valid_df = pd.read_csv(\"experimental/valid_negations_fliped.csv\", usecols = fields)\n",
    "test_df = pd.read_csv(\"data/pre_processed/test_lemma_nopunct_cleaned_sentencized.csv\", usecols = fields)\n",
    "test_df.dropna(inplace = True)\n",
    "\n",
    "\n",
    "#train_df = train_df[:100]\n",
    "#valid_df = valid_df[:100]\n",
    "\n",
    "train = list(zip(list(train_df['comment_text']), list(train_df['target'])))\n",
    "valid = list(zip(list(valid_df['comment_text']), list(valid_df['target'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(train_events, train_labels, valid_events, valid_labels, valid_df, featurizer, plot = False):\n",
    "    print('training...')\n",
    "    start = time.time()\n",
    "    model = svm.LinearSVC(max_iter = 10000)\n",
    "    model.fit(train_events, train_labels)\n",
    "    end = time.time()\n",
    "    print(\"Training took:\", int((end-start)), \"sec\")\n",
    "\n",
    "    start = time.time()\n",
    "    predicted_labels = model.predict(valid_events)\n",
    "    end = time.time()\n",
    "    print(\"Predicting took:\", int((end-start)), \"sec\")\n",
    "\n",
    "    evaluate(predicted_labels, valid_labels, valid_df, model, featurizer, plot)\n",
    "    return featurizer, train_events, train_labels, valid_events, valid_labels, predicted_labels, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_train_evaluate(train, valid, valid_df, verbose = False, plot = False):\n",
    "    print('featurizing train...')\n",
    "    featurizer = Featurizer()\n",
    "    start = time.time()\n",
    "    train_events, train_labels = featurizer.featurize(train, allow_new_features=True, verbose = verbose)\n",
    "    print('featurizing valid...')\n",
    "    valid_events, valid_labels = featurizer.featurize(valid, allow_new_features=False, verbose = verbose)\n",
    "    end = time.time()\n",
    "    print(\"Vectorizing took:\", int((end-start)), \"sec\")\n",
    "    \n",
    "    return train_and_evaluate(train_events, train_labels, valid_events, valid_labels, valid_df, featurizer, plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF scores for word monograms and bigrams as a baseline for adding additional features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "Training took: 9 sec\n",
      "Predicting took: 0 sec\n",
      "   label   n_true  n_tagged    precision           recall              F-score\n",
      "       1    28788    25360     0.66 / 0.66         0.58 / 0.58         0.61 / 0.61    \n",
      "       0   332306   335734     0.96 / 0.96         0.97 / 0.97         0.97 / 0.97    \n",
      "\n",
      "      Acc: 0.9422 / 0.9422\n",
      "   Av.rec: 0.5172 / 0.5169\n",
      "     AUC : 0.8099 / 0.8101\n",
      "-----------------------------\n",
      "From 20874 incorrectly classified, 6403 are ambiguous ( 30.674523330458943 %)\n",
      "saved (non-ambiguous) examples for correctly and incorrectly classified comments to /examples\n"
     ]
    }
   ],
   "source": [
    "#Testing showed that there seems to be no improvement when using more than bigrams\n",
    "#tfidf_vectorizer = TfidfVectorizer(ngram_range = (1,2))\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range = (1,2), token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'\")\n",
    "\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(train_df['comment_text'])\n",
    "target_train = list(train_df[\"target\"])\n",
    "tfidf_valid = tfidf_vectorizer.transform(valid_df['comment_text'])\n",
    "valid_train = list(valid_df[\"target\"])\n",
    "\n",
    "res = train_and_evaluate(tfidf_train, target_train, tfidf_valid, valid_train, valid_df, tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare previous results to the same model but given data where negated words are prefixed with \"NEG_\" and the negation being removed. Also check how many classifications are corrected by doing that and how many are falsified and saving some examples to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "Training took: 7 sec\n",
      "Predicting took: 0 sec\n",
      "   label   n_true  n_tagged    precision           recall              F-score\n",
      "       1    28788    25303     0.66 / 0.66         0.58 / 0.58         0.61 / 0.61    \n",
      "       0   332306   335791     0.96 / 0.96         0.97 / 0.97         0.97 / 0.97    \n",
      "\n",
      "      Acc: 0.9422 / 0.9422\n",
      "   Av.rec: 0.5169 / 0.5169\n",
      "     AUC : 0.8101 / 0.8101\n",
      "-----------------------------\n",
      "From 20871 incorrectly classified, 6403 are ambiguous ( 30.678932490057974 %)\n",
      "saved (non-ambiguous) examples for correctly and incorrectly classified comments to /examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-42-eba3d78e42a8>:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  differently_classified[\"normal\"] = predicted_labels[difference_msk]\n",
      "<ipython-input-42-eba3d78e42a8>:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  differently_classified[\"flipped\"] = predicted_labels1[difference_msk]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preventive maintenance be cheap than let infrastructure fall apart . Maybe the voter be NEG_idiot after all . \n",
      "\n",
      "I smell a rat . This make NEG_sense . Of course they spy on each other . Of course the hack each other . But Putin be a NEG_stupid man . So this make NEG_sense . -PRON- have a set up . I can NEG_stand Trump but -PRON- would be with him on this in doubt it . But it would effectively tie Trump up with regard to Russia . The military Industrial complex would NEG_want to end the cold war with Russia a it mean arm and money for them . \n",
      "\n",
      "Please provide example and of this so call over reach on state hunt rule . At that point -PRON- will assume your 're a NEG_idiotic a you sound . \n",
      "\n",
      "Jerry No . You be wrong . What you say would be like prohibit a victim of kidnap from bring charge after they pay a ransom . A victim have every right to press charge for themselves . do be NEG_stupid . \n",
      "\n",
      "The youth of ANC be thankfully a NEG_stupid a Ramaphosa seem to think . They be aware of the situation in SA and be sensible enough to NEG_vote for him . \n",
      "\n",
      "O nice Walker be yet again spend OUR money to have Himself a feast . Liketo hea how he be go to justify this O I forget he do NEG_have to justify anything he doe . He be a big reason our debt never go down it just continue to increase . Someday we the people of Alaska be go to over throw the friggin politician and take back our state and than our state will be run by common sense people NEG_idiot like Walker and Mouthkowski and Young and the rest of the lazy lie self serve money monger . \n",
      "\n",
      "Out of the current group of gop this be the only that be a NEG_idiot . \n",
      "\n",
      "Read the DP . This existential crisis rank far behind baseball football marijuana fake scandal letter to lovelorn columnist etc . for the DP . You seem like a very good inform person Burey . So that have good . Trump be NEG_stupid . Mattis McMaster et al have make him very aware of the situation . But Trump be NEG_go to allow a certifiable lunatic who just threaten Guam to do continue with his WMD brinksmanship and negotiation at this stage be a NEG_option . War perhaps WW II be the much likely outcome to this crisis the vast majority of people be definitely NEG_take this seriously . Good luck to you Burey . \n",
      "\n",
      "By the Constitution of US pot be legally equal for everyone because of the 14th Amendment . This Amendment be quash by the imperialist attorney caste with Prohibition to combat the notion that US citizen be free which mean that the interest they have in ownership of young male for war would be NEG_challenge . In return woman be give the vote . Like President Lincoln say you can fool some of the people all of the time you can even fool all of the people some of the time but you can NEG_fool all of the people all of the time . -PRON- be slave without property ownership right and we be criminalize when we try to exercise that right . If you do NEG_like it you be NEG_alone though and I think that be why we now have Harriet Tubman on the money . \n",
      "\n",
      "Caldwell be now pander to the voter . Fortunately they be NEG_stupid . He will say whatever he think will get him vote and then do whatever he want . Time for new leadership at Honolulu Hale . \n",
      "\n",
      "Does anyone believe that it be possible for Mr. Browder to make billion in Russia without pay bribe and commit other type of crime . After all the US have recall his visa a couple of day ago . People in the US Department Of State be NEG_stupid . \n",
      "\n",
      "Pat Tabler I mean I know his job a a Roger employee be to talk up the team own by the guy who sign his pay cheque but fan be NEG_idiot . Tulowitzki make a bad throw to 1st late in yesterday have game a throw he shoulda never make a he have chance to get the runner throw wildly and Tabler go into damage control mode shovel lionized praise on the 1st baseman for come off the bag to get the throw . The right comment be to get on Tulowitzki for make the throw at all . Monday have game a laugher he conclude by say every Your Js player have a hand in the offence . I guess -PRON- have include Pillar have reach 1st on a FC during his for contribution . And today Martin fall asleep at 2nd and get pick off a horrendous mistake and rather than dump on Martin a he do with Hamilton have stupid attempt to steal 3rd yesterday he extol the s for make a great play . Uh NEG_. Martin mess up forget the s . \n",
      "\n",
      "Its NEG_stupid at all . Its a comment that help to show that thing need to be keep in perspective . \n",
      "\n",
      "Please stop use the Washington Post for story . This news outlet be bias . Some other be the Huffington Post and Mother Jones . We be NEG_stupid and easily recognize bias side report when we see it . ADN should at little try to find the middle . \n",
      "\n",
      "do be NEG_fool Weed be NEG_the answer . It be a drug and it be addictive . Sure it can give you some insight some epiphany in the short term but after a while it narrow our mind and your perspective . The problem with legalization and all this talk be that it make it seem like wee be a answer . It be NEG_. Like booze it be old school and for fool . Better take the new path to a clear high smart drug . http starthere . \n",
      "\n",
      "The US and some of its ally simply can NEG_live with the fact that other nation can and will acquire nuclear capability if they so desire . Those day when the US can intimidate and threaten other state be long go and such day will NEG_come back . It be a simple matt of military hegemony . The US want to exert and assert its dominance on country that it label rogue nation . Granted Little Kim be brace for a fight a fight that he know will be NEG_foolishly accept by other because in a nuclear confrontation there be NEG_winner . Kim hold the triumph card and he know it . The US and its ally should accept the reality albeit with great reluctance that North Korea be a nuclear power and should therefore be treat a with respect . Sanctions do NEG_work period . It only embolden and empower the masse for they will see in it a bully that be the US . Demonizing Little Kim be childish . Kim want recognition . He be NEG_stupid . Stop treat him like a fool . \n",
      "\n",
      "In fall swoop the clueless Lauren Byrne mother to a innocent daughter and a future biter rapist set the Newfies be NEG_fool argument back decade . \n",
      "\n",
      "do be NEG_fool this domestic embarrassment get fully beat by the Tesla family sedan . time Dodge verify by nobody and get this with the SEATS REMOVED lol . Tesla MotorTrend http 2lgGcvu RoadAndTrack 2.28s http 2qLmSZu Kuniskis nonsense re rollout be a fool the gullible lipstick a pig tactic . Rollout mean already roll which of course be NOT a time it be a . . . I do NEG_know and neither doe he since it vary with the track temperature time of day weather . He can pick whatever rollout he want the Tesla will still win . \n",
      "\n",
      "Palestinian Arab simply can NEG_afford establish peace with Israel . In the event of peace the war money from Iran and Arab Leagues that PA Hamas get will be go UNWRA money will be go and EU US Canadian money will be decrease because it will be NEG_a war zone anymore . Palestinian Arab be NEG_idiot to lose billion of dollar they get every year for sign a useless peace of paper because Israel doe NEG_attack them anyway . \n",
      "\n",
      "I do NEG_understand why Trump have to do anything let North Korea do what they want if they EVER fire a nuclear missile at South Korea Japan Guam or the US mainland they will be completely destroy regime and all the North Korean government include Fat Boy want to survive above anything else that be their top priority even they would be NEG_stupid enough to use their missile in a pre emptive scenario . \n",
      "\n",
      "I do NEG_agree with liberal often because -PRON- be a NEG_idiot and because I love America but when they once again say We must have a conversation about gun . I still could NEG_agree much . And since all -PRON- have hear be you leftist shriek at us all week -PRON- will start it off . You do NEG_ever get to disarm us . Not ever . https columnist kurtschlichter nothing make liberal angry than us normal insist on our right n2390586 ? utm_source thdaily & utm_medium email & utm_campaign nl & newsletterad . \n",
      "\n",
      "so if we be go to allow snide derogatory remark about the formenr PM have perceive weight issue can we also unleash on the current leader have feminine character . That he have a lisp and clearly pretend to be heterosexual but can NEG_fool the real man in the room . \n",
      "\n",
      "for all comments where either 'idiot', 'fool' or 'stupid' was flipped:\n",
      "36 where corrected by flipping and 22 where falsified by flipping \n",
      "in total, 803 where corrected and 800 where falsified\n",
      "0.4439287276997125 % of comments where differently classified\n"
     ]
    }
   ],
   "source": [
    "#if (1 == 0):\n",
    "fields = ['comment_text', 'toxicity','target']\n",
    "\n",
    "train_df = pd.read_csv(\"data/pre_processed/_NEG_for_negations/train_negations_fliped.csv\", usecols = fields)\n",
    "train_df = train_df[:int(len(train_df) * 1/4)]\n",
    "valid_df = pd.read_csv(\"data/pre_processed/_NEG_for_negations/valid_negations_fliped.csv\", usecols = fields)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range = (1,2))\n",
    "\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(train_df['comment_text'])\n",
    "target_train = list(train_df[\"target\"])\n",
    "tfidf_valid = tfidf_vectorizer.transform(valid_df['comment_text'])\n",
    "valid_train = list(valid_df[\"target\"])\n",
    "\n",
    "res1 = train_and_evaluate(tfidf_train, target_train, tfidf_valid, valid_train, valid_df, tfidf_vectorizer)\n",
    "\n",
    "featurizer, train_events, train_labels, valid_events, valid_labels, predicted_labels, model = res\n",
    "featurizer1, train_events1, train_labels1, valid_events1, valid_labels1, predicted_labels1, model1 = res1\n",
    "\n",
    "difference_msk = [ x != y for (x,y) in zip(predicted_labels, predicted_labels1)]\n",
    "differently_classified = valid_df.loc[difference_msk]\n",
    "\n",
    "differently_classified[\"normal\"] = predicted_labels[difference_msk]\n",
    "differently_classified[\"flipped\"] = predicted_labels1[difference_msk]\n",
    "\n",
    "differently_classified.to_csv(\"examples/differently_classified.csv\")\n",
    "\n",
    "\n",
    "negated_insults = [\"NEG_idiot\", \"NEG_fool\", \"NEG_stupid\"]\n",
    "neg_insults_corrected = 0\n",
    "neg_insults_falsified = 0\n",
    "neg_total_corrected = 0\n",
    "neg_total_falsified = 0\n",
    "\n",
    "for index, row in differently_classified.iterrows():\n",
    "    #print(row['comment_text'])\n",
    "    if any(x in row['comment_text'] for x in negated_insults):\n",
    "        if ((row[\"target\"] == row[\"normal\"]) and (row[\"target\"] != row[\"flipped\"])):\n",
    "            neg_insults_falsified += 1\n",
    "        if ((row[\"target\"] != row[\"normal\"]) and (row[\"target\"] == row[\"flipped\"])):\n",
    "            neg_insults_corrected += 1\n",
    "    if ((row[\"target\"] == row[\"normal\"]) and (row[\"target\"] != row[\"flipped\"])):\n",
    "            neg_total_falsified += 1\n",
    "    if ((row[\"target\"] != row[\"normal\"]) and (row[\"target\"] == row[\"flipped\"])):\n",
    "            neg_total_corrected += 1\n",
    "\n",
    "print(\"for all comments where either 'idiot', 'fool' or 'stupid' was flipped:\\n\" + str(neg_insults_corrected), \\\n",
    "     \"where corrected by flipping and\", neg_insults_falsified , \"where falsified by flipping\", \"\\nin total,\" ,\\\n",
    "     neg_total_corrected, \"where corrected and\", neg_total_falsified, \"where falsified\\n\" + \\\n",
    "     str((len(differently_classified)/len(valid_df)) * 100), \"% of comments where differently classified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding additional features on top of the TFIDF values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%...3%...6%...8%...11%...14%...17%...19%...22%...25%...28%...30%...33%...36%...39%...42%...44%...47%...50%...53%...55%...58%...61%...64%...66%...69%...72%...75%...78%...80%...83%...86%...89%...91%...94%...97%...100%...done!\n",
      "0%...3%...6%...8%...11%...14%...17%...19%...22%...25%...28%...30%...33%...36%...39%...42%...44%...47%...50%...53%...55%...58%...61%...64%...66%...69%...72%...75%...78%...80%...83%...86%...89%...91%...94%...97%..."
     ]
    }
   ],
   "source": [
    "#set additional features\n",
    "Featurizer.feature_functions = ['char_1grams', 'char_2grams']\n",
    "\n",
    "#create featurizer who's dictionary is based on the tfidf-vectorizers dictionary\n",
    "custom_featurizer = Featurizer(foreign_features = tfidf_vectorizer.vocabulary_.copy())\n",
    "train_events, train_labels = custom_featurizer.featurize(train, allow_new_features=True, verbose = True)\n",
    "valid_events, valid_labels = custom_featurizer.featurize(valid, allow_new_features=False, verbose = True)\n",
    "\n",
    "#combine outputs of tfidf and custom featurizer\n",
    "train_tfidf_custom = custom_featurizer.merge_feature_matrices(tfidf_train, train_events)\n",
    "valid_tfidf_custom = custom_featurizer.merge_feature_matrices(tfidf_valid, valid_events)\n",
    "\n",
    "res = train_and_evaluate(train_tfidf_custom, train_labels, valid_tfidf_custom, valid_labels, valid_df, custom_featurizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Printing featurized version of a comment to check if everything went right \n",
    "custom_featurizer.print_sample_with_feature_names(train_tfidf_custom, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A cell for trying stuff out\n",
    "\n",
    "#custom_featurizer.print_sample_with_feature_names(train_tfidf_custom, 1)\n",
    "featurizer, train_events, train_labels, valid_events, valid_labels, predicted_labels, model = res\n",
    "#te = tfidf_vectorizer.transform(['Wow . be a hate fill monster .'])\n",
    "te = tfidf_vectorizer.transform(['kill them chop them up and sell off the part .'])\n",
    "\n",
    "model.predict(te)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
