{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D #,CuDNNLSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "\n",
    "EMBEDDING_FILES = [\n",
    "    'input/gensim-embeddings-dataset/crawl-300d-2M.vec',\n",
    "    'input/gensim-embeddings-dataset/glove.840B.300d.txt'\n",
    "]\n",
    "NUM_MODELS = 2\n",
    "BATCH_SIZE = 16\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 4\n",
    "MAX_LEN = 220\n",
    "IDENTITY_COLUMNS = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n",
    "]\n",
    "AUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "TARGET_COLUMN = 'target'\n",
    "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    #each line in the file looks like \n",
    "    # apple 0.3 0.4 0.5 0.6 ...\n",
    "    # that is a word followed by 300 float numbers\n",
    "\n",
    "    with open(path) as f:\n",
    "        #return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "        return dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(f))\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    #print(type(embedding_index))\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        for candidate in [word, word.lower()]:\n",
    "            if candidate in embedding_index:\n",
    "                embedding_matrix[i] = embedding_index[candidate]\n",
    "                break\n",
    "    return embedding_matrix\n",
    "    \n",
    "\n",
    "def build_model(embedding_matrix, num_aux_targets):\n",
    "    words = Input(shape=(None,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=[result, aux_result])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1999996it [01:41, 19769.61it/s]\n",
      "2196017it [01:45, 20867.61it/s]\n"
     ]
    }
   ],
   "source": [
    "#train_df = pd.read_csv('input/jigsaw-unintended-bias-in-toxicity-classification/train_custom.csv')\n",
    "#test_df = pd.read_csv('input/jigsaw-unintended-bias-in-toxicity-classification/valid_custom.csv')\n",
    "train_df = pd.read_csv('input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test_df = pd.read_csv('input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "\n",
    "#print(len(train_df))\n",
    "#print(len(test_df))\n",
    "#train_df.dropna(inplace = True, subset = ['comment_text'])\n",
    "#test_df.dropna(inplace = True, subset = ['comment_text'])\n",
    "#print(len(train_df))\n",
    "#print(len(test_df))\n",
    "#train_df = train_df[:int(len(train_df) * 1/4)]\n",
    "#train_df = train_df[:10000]\n",
    "#test_df = test_df[:10000]\n",
    "\n",
    "x_train = train_df[TEXT_COLUMN].astype(str)\n",
    "y_train = train_df[TARGET_COLUMN].values\n",
    "y_aux_train = train_df[AUX_COLUMNS].values\n",
    "x_test = test_df[TEXT_COLUMN].astype(str)\n",
    "#print(y_train[:10])\n",
    "\n",
    "for column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n",
    "    train_df[column] = np.where(train_df[column] >= 0.5, True, False)\n",
    "\n",
    "tokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE, lower=False)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n",
    "\n",
    "sample_weights = np.ones(len(x_train), dtype=np.float32)\n",
    "sample_weights += train_df[IDENTITY_COLUMNS].sum(axis=1)\n",
    "sample_weights += train_df[TARGET_COLUMN] * (~train_df[IDENTITY_COLUMNS]).sum(axis=1)\n",
    "sample_weights += (~train_df[TARGET_COLUMN]) * train_df[IDENTITY_COLUMNS].sum(axis=1) * 5\n",
    "sample_weights /= sample_weights.mean()\n",
    "    \n",
    "embedding_matrix = np.concatenate(\n",
    "    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n",
    "\n",
    "checkpoint_predictions = []\n",
    "weights = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(embedding_matrix, y_aux_train.shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting epoch 0\n",
      "112805/112805 - 4765s - loss: 0.5212 - dense_2_loss: 0.4163 - dense_3_loss: 0.1049\n",
      "finishing epoch 0\n",
      "starting epoch 1\n",
      "112805/112805 - 4772s - loss: 0.5118 - dense_2_loss: 0.4088 - dense_3_loss: 0.1030\n",
      "finishing epoch 1\n",
      "starting epoch 2\n",
      "112805/112805 - 4641s - loss: 0.5100 - dense_2_loss: 0.4072 - dense_3_loss: 0.1028\n",
      "finishing epoch 2\n",
      "starting epoch 3\n",
      "112805/112805 - 4742s - loss: 0.5091 - dense_2_loss: 0.4064 - dense_3_loss: 0.1027\n",
      "finishing epoch 3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for model_idx in range(NUM_MODELS):\n",
    "for global_epoch in range(EPOCHS):\n",
    "    print(\"starting epoch\", global_epoch)\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        [y_train, y_aux_train],\n",
    "        batch_size=16,\n",
    "        epochs=1,\n",
    "        verbose=2,\n",
    "        sample_weight=[sample_weights.values, np.ones_like(sample_weights)]\n",
    "    )\n",
    "    print(\"finishing epoch\", global_epoch)\n",
    "    checkpoint_predictions.append(model.predict(x_test, batch_size=16)[0].flatten())\n",
    "    weights.append(2 ** global_epoch)\n",
    "\n",
    "predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n",
    "\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_df.id,\n",
    "    'prediction': predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'weakref' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4cf6978a4f86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"model.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: cannot pickle 'weakref' object"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
